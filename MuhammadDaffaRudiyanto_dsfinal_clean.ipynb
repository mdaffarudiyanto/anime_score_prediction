{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Data Science Final Project</h1>\n",
    "<h3>Muhammad Daffa Rudiyanto 26002104783</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Setup</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from sklearn.svm import SVR\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import transformers\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch import cuda\n",
    "from torch.utils.data import TensorDataset, Dataset, DataLoader\n",
    "from transformers import BertTokenizer, BertModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 4\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "if cuda.is_available():\n",
    "    cuda.manual_seed(SEED)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r\"MuhammadDaffaRudiyanto_animedataset.csv\")\n",
    "print(len(df))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Data Preprocessing</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check number of missing value\n",
    "missing_values = df.isna().sum()\n",
    "rows_with_missing_value = df.isna().any(axis=1).sum()\n",
    "\n",
    "print(missing_values)\n",
    "print(\"Numbers of rows with at least one missing data:\", rows_with_missing_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rows_with_empty_values = df[df[['Studios', 'Score']].isna().all(axis=1)].shape[0]\n",
    "\n",
    "# print(\"Number of data points with empty values in all specified columns:\", rows_with_empty_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(df['Score'], bins=10, edgecolor='black')\n",
    "plt.title('Histogram of Anime Scores')\n",
    "plt.xlabel('Score')\n",
    "plt.ylabel('Frequency')\n",
    "plt.ylim(0, 5000)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping rows with NaN values in specific columns\n",
    "df = df.dropna(subset=['Score', 'Synopsis', 'Genres', 'Ratings', 'Studios'])\n",
    "\n",
    "# Filtering out specific categories and values\n",
    "df = df[~df['Type'].isin(['Music', 'Special', 'Unknown']) & \n",
    "        (df['Ratings'] != \"Rx - Hentai\") & \n",
    "        ~df['Status'].isin([\"Currently Airing\", \"Not yet aired\"])]\n",
    "\n",
    "df['Members'] = df['Members'].astype(float)\n",
    "\n",
    "\n",
    "# df = df.drop(['Title', 'Status'], axis=1)\n",
    "bins = [0, 5, 7, float('inf')]  # Start bins from 0\n",
    "labels = [0, 1, 2]  # Labels for the bins\n",
    "df['bins_score'] = pd.cut(df['Score'], bins=bins, labels=labels, right=False)\n",
    "df['bins_score'] = df['bins_score'].astype(float)\n",
    "\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_values = df.isna().sum()\n",
    "print(missing_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(df['Score'], bins=10, edgecolor='black')\n",
    "plt.title('Histogram of Anime Scores')\n",
    "plt.xlabel('Score')\n",
    "plt.ylabel('Frequency')\n",
    "plt.ylim(0, 5000)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sample(frac=1, random_state=SEED)\n",
    "df = df.reset_index(drop=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Machine Learning</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(lowercase=True)\n",
    "\n",
    "# Transform Synopsis into a sparse matriax\n",
    "synopsis_list = vectorizer.fit_transform(df['Synopsis'])\n",
    "\n",
    "# Process Genres, Studios, Type, and Ratings\n",
    "# Convert them to sparse matrices after getting dummies\n",
    "genres_list = csr_matrix(df['Genres'].str.get_dummies(sep=', '))\n",
    "studios_list = csr_matrix(df['Studios'].str.get_dummies(sep=', '))\n",
    "type_encoded = csr_matrix(pd.get_dummies(df['Type'], prefix='Type')).astype('float32')\n",
    "ratings_encoded = csr_matrix(pd.get_dummies(df['Ratings'], prefix='Ratings')).astype('float32')\n",
    "\n",
    "# Combine everything into a single sparse matrix\n",
    "X = hstack([synopsis_list, genres_list, studios_list, type_encoded, ratings_encoded])\n",
    "\n",
    "# Target variable\n",
    "y = df['Score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_model = SVR(kernel='rbf', verbose=True)\n",
    "ml_model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "predictions = ml_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean Squared Error\n",
    "mse = mean_squared_error(y_test, predictions)\n",
    "print(\"Mean Squared Error: %.3f\" % mse)\n",
    "\n",
    "# Root Mean Squared Error\n",
    "rmse = np.sqrt(mse)\n",
    "print(\"Root Mean Squared Error: %.3f\" % rmse)\n",
    "\n",
    "mae = mean_absolute_error(y_test, predictions)\n",
    "print(\"Mean Absolute Error: %.3f\" % mae) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Deep Learning Part 1: Fine-tuning BERT for Synopsis</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAX_LEN = 200\n",
    "MAX_LEN = 512\n",
    "TRAIN_BATCH_SIZE = 8\n",
    "VALID_BATCH_SIZE = 8\n",
    "EPOCHS = 10\n",
    "LEARNING_RATE = 1e-05\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased',  do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "\n",
    "    def __init__(self, dataframe, tokenizer, max_len):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = dataframe\n",
    "        self.Synopsis = dataframe.Synopsis\n",
    "        self.targets = self.data.Score\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.Synopsis)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        Synopsis = str(self.Synopsis[index])\n",
    "        Synopsis = \" \".join(Synopsis.split())\n",
    "\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            Synopsis,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            pad_to_max_length=True,\n",
    "            return_token_type_ids=True\n",
    "        )\n",
    "        ids = inputs['input_ids']\n",
    "        mask = inputs['attention_mask']\n",
    "        token_type_ids = inputs[\"token_type_ids\"]\n",
    "\n",
    "\n",
    "        return {\n",
    "            'ids': torch.tensor(ids, dtype=torch.long),\n",
    "            'mask': torch.tensor(mask, dtype=torch.long),\n",
    "            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n",
    "            'targets': torch.tensor(self.targets[index], dtype=torch.float)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = df.loc[:, ['Synopsis', 'Score']] \n",
    "new_df = new_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the dataset and dataloader for the neural network\n",
    "\n",
    "# Define the size of the training set\n",
    "train_size = 0.8\n",
    "\n",
    "# Perform a stratified split to maintain the distribution of 'bins_score' in both sets\n",
    "train_dataset, test_dataset = train_test_split(new_df, train_size=train_size, stratify=df['bins_score'], random_state=SEED)\n",
    "\n",
    "# Reset index for both datasets\n",
    "train_dataset = train_dataset.reset_index(drop=True)\n",
    "test_dataset = test_dataset.reset_index(drop=True)\n",
    "\n",
    "\n",
    "print(\"FULL Dataset: {}\".format(new_df.shape))\n",
    "print(\"TRAIN Dataset: {}\".format(train_dataset.shape))\n",
    "print(\"TEST Dataset: {}\".format(test_dataset.shape))\n",
    "\n",
    "training_set = CustomDataset(train_dataset, tokenizer, MAX_LEN)\n",
    "testing_set = CustomDataset(test_dataset, tokenizer, MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_params = {'batch_size': TRAIN_BATCH_SIZE,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "test_params = {'batch_size': VALID_BATCH_SIZE,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "training_loader = DataLoader(training_set, **train_params)\n",
    "testing_loader = DataLoader(testing_set, **test_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the customized model, by adding a drop out and a dense layer on top of distil bert to get the final output for the model.\n",
    "\n",
    "class BERTClass(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BERTClass, self).__init__()\n",
    "        self.l1 = transformers.BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.l2 = torch.nn.Dropout(0.3)\n",
    "        self.l3 = torch.nn.Linear(768, 1)  # Output a single continuous value\n",
    "\n",
    "    def forward(self, ids, mask, token_type_ids):\n",
    "        _, output_1= self.l1(ids, attention_mask=mask, token_type_ids=token_type_ids, return_dict=False)\n",
    "        output_2 = self.l2(output_1)\n",
    "        output = self.l3(output_2)\n",
    "        return output\n",
    "\n",
    "model = BERTClass()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(outputs, targets):\n",
    "    return torch.nn.MSELoss()(outputs.view(-1), targets.view(-1))  # Mean Squared Error Loss\n",
    "optimizer = torch.optim.Adam(params =  model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "    fin_targets_train = []\n",
    "    fin_outputs_train = []\n",
    "    for _, data in enumerate(training_loader, 0):\n",
    "        ids = data['ids'].to(device, dtype=torch.long)\n",
    "        mask = data['mask'].to(device, dtype=torch.long)\n",
    "        token_type_ids = data['token_type_ids'].to(device, dtype=torch.long)\n",
    "        targets = data['targets'].to(device, dtype=torch.float)\n",
    "\n",
    "        outputs = model(ids, mask, token_type_ids)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss = loss_fn(outputs, targets)\n",
    "\n",
    "        fin_targets_train.extend(targets.cpu().detach().numpy().tolist())\n",
    "        fin_outputs_train.extend(outputs.cpu().detach().numpy().tolist())\n",
    "\n",
    "        if _ % 5000 == 0:\n",
    "            print(f'Epoch: {epoch}, Loss:  {loss.item()}')\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    return fin_outputs_train, fin_targets_train\n",
    "\n",
    "def validation(epoch):\n",
    "    model.eval()\n",
    "    fin_targets = []\n",
    "    fin_outputs = []\n",
    "    with torch.no_grad():\n",
    "        for _, data in enumerate(testing_loader, 0):\n",
    "            ids = data['ids'].to(device, dtype=torch.long)\n",
    "            mask = data['mask'].to(device, dtype=torch.long)\n",
    "            token_type_ids = data['token_type_ids'].to(device, dtype=torch.long)\n",
    "            targets = data['targets'].to(device, dtype=torch.float)\n",
    "\n",
    "            outputs = model(ids, mask, token_type_ids)\n",
    "            fin_targets.extend(targets.cpu().detach().numpy().tolist())\n",
    "            fin_outputs.extend(outputs.cpu().detach().numpy().tolist())\n",
    "\n",
    "    return fin_outputs, fin_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main training and validation loop\n",
    "for epoch in range(EPOCHS):\n",
    "    outputs_train, targets_train = train(epoch)\n",
    "    outputs_train = np.squeeze(outputs_train)  # Convert outputs to a 1D array\n",
    "\n",
    "    # Calculate and print training metrics\n",
    "    mse_train = mean_squared_error(targets_train, outputs_train)\n",
    "    mae_train = mean_absolute_error(targets_train, outputs_train)\n",
    "    print(f\"Epoch {epoch} - Training Mean Squared Error: {mse_train}\")\n",
    "    print(f\"Epoch {epoch} - Training Mean Absolute Error: {mae_train}\")\n",
    "\n",
    "    # Validate the model\n",
    "    outputs, targets = validation(epoch)\n",
    "    outputs = np.squeeze(outputs)  # Convert outputs to a 1D array for regression metrics\n",
    "\n",
    "    # Calculate and print validation metrics\n",
    "    mse = mean_squared_error(targets, outputs)\n",
    "    mae = mean_absolute_error(targets, outputs)\n",
    "    r2 = r2_score(targets, outputs)\n",
    "    print(f\"Epoch {epoch} - Validation Mean Squared Error: {mse}\")\n",
    "    print(f\"Epoch {epoch} - Validation Mean Absolute Error: {mae}\")\n",
    "    print(f\"Epoch {epoch} - R-Squared: {r2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'fine_tuned_bert_model_3.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Deep Learning Part 2.1: Extract Embeddings from Fine-tuned BERT Model</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTClass(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BERTClass, self).__init__()\n",
    "        self.l1 = transformers.BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.l2 = torch.nn.Dropout(0.3)\n",
    "        self.l3 = torch.nn.Linear(768, 1)\n",
    "\n",
    "    def forward(self, ids, mask, token_type_ids):\n",
    "        output = self.l1(ids, attention_mask=mask, token_type_ids=token_type_ids, return_dict=False)\n",
    "        pooled_output = output[1]  # This is the embedding\n",
    "        return pooled_output  # Return just the embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "bert_model = BERTClass()\n",
    "bert_model.load_state_dict(torch.load('fine_tuned_bert_model_3.pth'))\n",
    "bert_model.to(device)\n",
    "\n",
    "# Set to evaluation mode for inference\n",
    "bert_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_bert_embeddings(data_loader, model):\n",
    "    model.eval()  # Make sure the model is in evaluation mode\n",
    "    embeddings = []\n",
    "    with torch.no_grad():\n",
    "        for _, data in enumerate(data_loader, 0):\n",
    "            ids = data['ids'].to(device, dtype=torch.long)\n",
    "            mask = data['mask'].to(device, dtype=torch.long)\n",
    "            token_type_ids = data['token_type_ids'].to(device, dtype=torch.long)\n",
    "\n",
    "            embedding = model(ids, mask, token_type_ids)\n",
    "            embeddings.append(embedding.cpu().numpy())\n",
    "\n",
    "    return np.vstack(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'df' is your DataFrame and it has a column named 'Synopsis'\n",
    "synopses = df['Synopsis'].tolist()  # Convert the column to a list\n",
    "\n",
    "# Initialize the BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SynopsesEmbeddingDataset(Dataset):\n",
    "    def __init__(self, synopses_series, tokenizer, max_len):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.synopses = synopses_series\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.synopses)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        synopsis = str(self.synopses.iloc[index])\n",
    "        synopsis = \" \".join(synopsis.split())\n",
    "\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            synopsis,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            pad_to_max_length=True,\n",
    "            return_attention_mask=True,\n",
    "            return_token_type_ids=True\n",
    "        )\n",
    "        return {\n",
    "            'ids': torch.tensor(inputs['input_ids'], dtype=torch.long),\n",
    "            'mask': torch.tensor(inputs['attention_mask'], dtype=torch.long),\n",
    "            'token_type_ids': torch.tensor(inputs['token_type_ids'], dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the dataset and dataloader\n",
    "synopses_dataset = SynopsesEmbeddingDataset(df['Synopsis'], tokenizer, max_len=512)\n",
    "synopses_data_loader = DataLoader(synopses_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "synopsis_embeddings = generate_bert_embeddings(synopses_data_loader, bert_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Deep Learning Part 2.2: Combine with other featrues (Genres, Studios, Rating, and Type)</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your existing code for processing features\n",
    "genres_list_dl = df['Genres'].str.get_dummies(sep=', ')\n",
    "studios_list_dl = df['Studios'].str.get_dummies(sep=', ')\n",
    "type_encoded_dl = pd.get_dummies(df['Type'], prefix='Type')\n",
    "ratings_encoded_dl = pd.get_dummies(df['Ratings'], prefix='Ratings')\n",
    "\n",
    "other_features_df = pd.concat([genres_list_dl, studios_list_dl, type_encoded_dl, ratings_encoded_dl], axis=1)\n",
    "other_features = other_features_df.values.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_features = np.concatenate((synopsis_embeddings, other_features), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_dl = combined_features\n",
    "y_dl = df['Score'].values.astype('float32') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_dl, y_dl, train_size=0.8, random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating datasets for DataLoader\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Loaders\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=10, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=10, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model definition\n",
    "class RegressionModel(nn.Module): # defines a new class RegressionModel which is a subclass of nn.Module, the base class for all neural network modules in PyTorch. This is a standard way to create a new neural network in PyTorch.\n",
    "    def __init__(self, input_size): # constructor method\n",
    "        super(RegressionModel, self).__init__() # initializes the superclass (nn.Module) from which RegressionModel is derived\n",
    "        self.fc1 = nn.Linear(input_size, 64) # a fully connected (linear) layer with input_size neurons as input and 64 neurons as output\n",
    "        self.relu = nn.ReLU() # a ReLU activation function to establish non-linearity to the model, which allows it to learn more complex relationships.\n",
    "        self.fc2 = nn.Linear(64, 32) # another fully connected layer that takes 64 inputs (the output of the previous layer) and outputs to 32 neurons\n",
    "        self.fc3 = nn.Linear(32, 1) # final fully connected layer takes 32 inputs and reduces it to a single output since this is a regression problem\n",
    "\n",
    "    def forward(self, x): #defines the forward pass of the network\n",
    "        x = self.relu(self.fc1(x)) # the input x is passed through the first linear layer (fc1) and then through the ReLU activation function. The result is then stored back in x.\n",
    "        x = self.relu(self.fc2(x)) # output of the previous step (now in x) is passed through the second linear layer (fc2) and again through the ReLU function.\n",
    "        x = self.fc3(x) # the output from the last step is passed through the third linear layer (fc3)\n",
    "        return x # returns the final output\n",
    "    \n",
    "regression_model = RegressionModel(X_train.shape[1])\n",
    "regression_model.to(device)  # Move the model to the specified device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and optimizer\n",
    "criterion = nn.L1Loss()\n",
    "optimizer = optim.Adam(regression_model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "for epoch in range(20):\n",
    "    total_loss = 0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        # Move data to device\n",
    "        X_batch, y_batch = data[0].to(device), data[1].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = regression_model(X_batch)\n",
    "        loss = criterion(outputs, y_batch.view(-1, 1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    # Print loss after each epoch\n",
    "    print(f'Epoch [{epoch+1}/20], Loss: {total_loss / len(train_loader)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "regression_model.eval()\n",
    "fin_targets = []\n",
    "fin_outputs = []\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        # Move data to device\n",
    "        X_batch, y_batch = data[0].to(device), data[1].to(device)\n",
    "        \n",
    "        outputs = regression_model(X_batch)\n",
    "        fin_targets.extend(y_batch.cpu().detach().numpy().tolist())\n",
    "        fin_outputs.extend(outputs.cpu().detach().numpy().tolist())\n",
    "\n",
    "    # Calculate and print validation MAE\n",
    "    mae = mean_absolute_error(fin_targets, fin_outputs)\n",
    "    print(f'Validation Mean Absolute Error: {mae}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-metal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
